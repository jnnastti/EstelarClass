#### ANOTAÇÕES - 03/02/2024

## TRATAR COLUNAS

# Kepler tem RA e Dec porem em deg, logo, deve ser calculado tbm em sexagesimal.
# Kepler e K2 precisam calcular as variações do  Stellar Effective Temperature.
# Mesma situação com as variações de Stellar Radius.
# Kepler e K2 não tem as variações do Planet Radius (Earth)
# Kepler e K2 não tem as  variações do Equilibrium.
# Kepler e K2 não tem as variações do log10.
# Kepler e K2 não tem as variações do Orbital Period.
# Kepler e K2 não tem as variações do Transit Duration e Depth.
# Kepler e K2 não tem as variações do campo Insolation Flux.

# Kepler nao tem a distance/stellar distance.
# K2 nao tem Transit Duration e Depth.
# Verificar o campo magnitude

#### ANOTAÇÕES - 07/03/2024

# Tratado a latitude e longitude do KEPLER para criar a coluna que possui os dados em sexagesimal.
# Não consegui calcular Stellar Effective Temperature, Stellar Radius,  Planet Radius (Earth) e Equilibrium.
# Feita a tentativa de calcular a distancia

#### ANOTAÇÕES - 11/03/2024

# Foi visto que não há necessidade de criar novas colunas com as variações dos dados.
# O campo magnitude é medido de maneiras diferentes em cada base, então recomenda-se utilizar somente quando 
  for feito o teste com as bases diferentes.
# Feito o Depth da missão K2

#### ANOTAÇÕES - 18/03/2024

# Acho que se investigar melhor, dá pra calcular o transit duration do K2
# Feita as funções para tratar valores nulos (imput de valor aleatorio, por média ou mediana)

#### ANOTAÇÕES - 27/03/2024

# Feita a tratativa da tipagem dos dados que estavam como object.
# Foi verificado se todas as colunas estão na mesma escala (fiquei com um pouco de duvida sobre o stellar_sur_gravity)
# Para esse mês, faltou o Escalonamento e boxplot
# Feita a classificação dos dados
# Por alguma razão a conversão de sexagesimal para segundos não funcionou (e será que precisa dessa coordenada sendo que já tem a normal?)

### ANOTAÇÕES - 01/04/2024

foi selecionado modelos de classificação mais classicos.  Optamos nao nao usar redes neurais: 1- base pequena, acreditando que, 
se obtido um bom resultado, cogitaremos novamente o uso da rede neural.

escalonar - utilizar o pre processing do sklearn. Porque os valores anteriores ficam salvos, assim é possivel inverter o escalonamento.
bloxplot - serve para ver os valores faltantes. 

### ANOTAÇÕES - 08/04/2024

Visto que os dados em sexagesimal nao sao necessarios.
Organização e separação dos tratamentos para cada base.

OBS: É importante tomar um pouco mais de cuidado com duas colunas: eq_temperatura e insol_flux, pois elas tem muito menos dados não nulos que as demais. 
     E a quantidade de nulos representa uma proporção bem grande em relação ao shape[0] do DF.

     Caso tenha formato próximo a uma distribuição normal, vale tentar imputar com valor aleatório baseado na média e desvio padrão. 
     Mas é importante fazer um comparativo de antes e depois da imputação para ver se o formato não mudou muito.

     No caso da insol_flux da K2, 50% dos dados tem valor de insol_flux igual ou menor que 68.55 (mediana). E a média é de 981.89. 
     Aqui não faz sentido imputar com valor aleatório baseado na média e desvio padrão. Entretanto, também não acho conveniente imputar 
     um valor único (mediana, nesse caso). Aqui vejo duas possibildades:

      1. Imputar por um valor aleatório de uma distribuição homogênea baseado nos valores da distância interquartil (16.60 e 234.31)
      2. Pensar em, primeiro, concatenar os 3 DFs para depois imputar os nulos. Assim teríamos mais dados para averiguar a distribuição 
         dos dados nas colunas que possuem faltantes e tomar uma decisão melhor baseada sobre a imputação.

Não foi necessario utilizar o boxplot.

escalonamento - Optamos por utilizar minMax pois a maioria dos dados sao nao normais. (assim como optamos por imputar por valores aleatórios 
                 dentro da distância interquartil pelo mesmo motivo).
                 Perde menos tempo e, como a base é relativamente pequena, acredito que não fará diferença significativa no desempenho do modelo.

                 Exploração de outros métodos de escalonamento da base ficará como proposição de trabalhos futuros.

### ANOTAÇÕES - 11/04/2024

Preciso de apoio com os outliers.
Tratar os NaN.

Escolha dos modelos de ML:
  Naive Bayes:  é conhecido por sua simplicidade e eficiência computacional. Ele assume independência condicional entre as características, 
  o que pode não ser verdadeiro em todos os casos, mas ainda assim tende a funcionar bem em conjuntos de dados com muitas características
   e uma quantidade moderada de dados.

  K-Vizinhos Mais Próximos (K-NN): O algoritmo K-NN é simples de entender e implementar. Ele faz parte da categoria de 
  "métodos baseados em instância", o que significa que não requer treinamento explícito, apenas memoriza os dados de treinamento.

  Árvores de Decisão: As árvores de decisão são altamente interpretáveis e podem lidar com uma variedade de tipos de dados, 
  incluindo numéricos e categóricos. Elas são capazes de capturar interações complexas entre as características e são menos 
  sensíveis a outliers em comparação com outros modelos. Além disso, as árvores de decisão naturalmente lidam com conjuntos 
  de dados desbalanceados e são robustas a dados ruidosos.

  Regressão Logística: A regressão logística é um modelo linear generalizado usado para problemas de classificação binária. 
  Ela é robusta e eficiente, especialmente quando a relação entre as características e a classe de destino é aproximadamente 
  linear. Além disso, fornece probabilidades associadas às previsões, o que pode ser útil para interpretar a confiança do modelo em suas previsões.

### ANOTAÇÕES - 12/04/2024

Corrigido a divisão das bases de teste e treinamento. os registros classificados como "CANDIDATE" serão utilizados somente quando tivermos o modelo final.
